# hns-migrator

(hive ns 迁移工具) Hive NameService Migrator

## Scripts

The `us_scripts` directory contains small utilities used during HDFS
namespace migrations.

### `get_table_location.sh`
Query Hive for the location of a table using `spark-sql --master local`.

```bash
./us_scripts/get_table_location.sh <database.table>
```

### `show_hdfs_owners.sh`
Display the owner and group for every level of a given HDFS path and
record the information to `hdfs_owners.log` in the `scripts` directory.

```bash
./us_scripts/show_hdfs_owners.sh <hdfs-path>
```

The output file contains lines of the form `path owner group` which can be
used by other utilities.

### `get_table_info.sh`
Convenience wrapper that combines the above steps and prints the table
location and ownership information.

```bash
./us_scripts/get_table_info.sh <database.table>
```

Each script exits with an error message if required arguments are not
provided.

### `verify_paths.sh`
Read the `hdfs_owners.log` file generated by `show_hdfs_owners.sh`, replace
`DClusterUS1` with the specified cluster name and output the `hadoop` commands
needed to create and set ownership for each missing path (up to the second to
last level).

```bash
./us_scripts/verify_paths.sh <new-cluster>
```

### `gen_part_set_location.sh`
Generate `ALTER TABLE ... PARTITION ... SET LOCATION` statements for all
partitions of a table, adjusting paths from `DClusterUS1` to `DClusterUS2`.

```bash
./us_scripts/gen_part_set_location.sh <database.table>
```

## 中文脚本说明与流程图

以下内容对仓库中的每个脚本进行了中文说明，并使用 Mermaid 绘制了相应的流程图，方便快速了解脚本的执行步骤。

### get_table_location.sh

该脚本通过 `spark-sql` 查询指定表的 `Location` 信息。

```mermaid
flowchart TD
    A[开始] --> B{检查参数}
    B -- 缺少参数 --> C[打印用法并退出]
    B -- 参数正确 --> D[运行 spark-sql]
    D --> E[解析 Location]
    E --> F[输出路径]
    F --> G[结束]
```

### show_hdfs_owners.sh

递归遍历指定的 HDFS 路径，输出各级目录的所有者和所属组，同时记录到 `hdfs_owners.log`。

```mermaid
flowchart TD
    A[开始] --> B{检查参数}
    B -- 缺少参数 --> C[打印用法并退出]
    B -- 参数正确 --> D[解析路径前缀]
    D --> E[遍历路径各层级]
    E --> F[查询 hadoop fs 信息]
    F --> G[记录 owner/group]
    G --> H{是否结束}
    H -- 否 --> E
    H -- 是 --> I[结束]
```

### get_table_info.sh

该脚本组合使用 `get_table_location.sh` 和 `show_hdfs_owners.sh`，先获取表路径，再查看 HDFS 目录的权限信息。

```mermaid
flowchart TD
    A[开始] --> B[调用 get_table_location.sh]
    B --> C{是否成功}
    C -- 否 --> D[报错并退出]
    C -- 是 --> E[打印表路径]
    E --> F[调用 show_hdfs_owners.sh]
    F --> G[结束]
```

### verify_paths.sh

根据 `hdfs_owners.log` 的内容，在目标集群上生成需要创建目录及设置权限的 `hadoop` 命令。

```mermaid
flowchart TD
    A[开始] --> B[读取 hdfs_owners.log]
    B --> C{文件是否存在}
    C -- 否 --> D[报错并退出]
    C -- 是 --> E[遍历日志行]
    E --> F[检查目录是否存在]
    F -- 不存在 --> G[输出 mkdir/chown]
    F -- 存在 --> H[跳过]
    G --> I{是否结束}
    H --> I
    I -- 否 --> E
    I -- 是 --> J[结束]
```

### gen_part_set_location.sh

该脚本查询某张分区表的所有分区，并生成将分区 `location` 从 `DClusterUS1` 调整到 `DClusterUS2` 的 `ALTER TABLE ... SET LOCATION` 语句，仅生成 SQL 不直接执行。

```mermaid
flowchart TD
    A[开始] --> B{检查参数}
    B -- 缺少参数 --> C[打印用法并退出]
    B -- 参数正确 --> D[获取数据库 Location]
    D --> E[构造目标库路径]
    E --> F[获取所有分区]
    F --> G[遍历分区]
    G --> H[查询分区当前 Location]
    H --> I{是否有效且属于 DClusterUS1}
    I -- 否 --> J[跳过该分区]
    I -- 是 --> K[计算新的路径]
    K --> L[输出 ALTER TABLE 语句]
    L --> M{是否还有分区}
    J --> M
    M -- 是 --> G
    M -- 否 --> N[结束]
```
